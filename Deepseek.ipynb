{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c1605006-fb58-410a-8f6d-743b2649d746",
   "metadata": {},
   "source": [
    "Explore how to interact with DeepSeek R1, an advanced open-source language model, using Python and the Together AI API. You‚Äôll learn how to securely manage API keys, make structured API calls, and generate AI-powered responses for different tasks. From retrieving text completions to generating code snippets and handling real-time streaming outputs, this hands-on project will equip you with essential skills to integrate AI into real-world applications. By the end, you‚Äôll understand how to fine-tune responses, process AI-generated outputs, and efficiently work with LLMs in your projects."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01e6b285-8c8b-4bb3-bff2-0c8cb593a500",
   "metadata": {},
   "source": [
    "## Objectives\n",
    "\n",
    "After completing this lab you will be able to:\n",
    "\n",
    "- Use **DeepSeek** to address a variety of real-world problems by integrating it into your projects.\n",
    "- Programmatically interact with the DeepSeek API for different use cases, from information retrieval to code generation.\n",
    "- Experiment with and modify parameters to generate custom answers tailored to diverse scenarios.\n",
    "- Leverage the flexibility of DeepSeek to explore and implement innovative solutions in your work.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04eac110-3965-413f-a308-17fd64076144",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "For this lab, we will be using the following libraries:\n",
    "\n",
    "*   [`python-dotenv`](https://pypi.org/project/python-dotenv/) for loading environment variables from a file (like `file.env`). It makes managing sensitive information (such as API keys) straightforward and secure.\n",
    "*   **`os`** `(built-in module)` for interacting with the operating system. We use it to access environment variables and other OS-level functions.\n",
    "*   [`together`](https://www.together.ai/) for providing a simple interface for interacting with the DeepSeek R1 API. It handles sending requests, receiving responses, and managing connection details so that you can easily utilize DeepSeek‚Äôs AI capabilities in your projects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c7303589-1c41-4a1b-b4a6-480f37598925",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting together==1.3.14\n",
      "  Downloading together-1.3.14-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.9.3 in c:\\users\\user\\anaconda3\\lib\\site-packages (from together==1.3.14) (3.10.5)\n",
      "Requirement already satisfied: click<9.0.0,>=8.1.7 in c:\\users\\user\\anaconda3\\lib\\site-packages (from together==1.3.14) (8.1.7)\n",
      "Collecting eval-type-backport<0.3.0,>=0.1.3 (from together==1.3.14)\n",
      "  Downloading eval_type_backport-0.2.2-py3-none-any.whl.metadata (2.2 kB)\n",
      "Requirement already satisfied: filelock<4.0.0,>=3.13.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from together==1.3.14) (3.13.1)\n",
      "Requirement already satisfied: numpy>=1.26.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from together==1.3.14) (1.26.4)\n",
      "Requirement already satisfied: pillow<11.0.0,>=10.3.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from together==1.3.14) (10.4.0)\n",
      "Requirement already satisfied: pyarrow>=10.0.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from together==1.3.14) (16.1.0)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.6.3 in c:\\users\\user\\anaconda3\\lib\\site-packages (from together==1.3.14) (2.10.5)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.31.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from together==1.3.14) (2.32.2)\n",
      "Collecting rich<14.0.0,>=13.8.1 (from together==1.3.14)\n",
      "  Downloading rich-13.9.4-py3-none-any.whl.metadata (18 kB)\n",
      "Requirement already satisfied: tabulate<0.10.0,>=0.9.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from together==1.3.14) (0.9.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.66.2 in c:\\users\\user\\anaconda3\\lib\\site-packages (from together==1.3.14) (4.66.5)\n",
      "Requirement already satisfied: typer<0.16,>=0.9 in c:\\users\\user\\anaconda3\\lib\\site-packages (from together==1.3.14) (0.15.1)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.9.3->together==1.3.14) (2.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\user\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.9.3->together==1.3.14) (1.2.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.9.3->together==1.3.14) (23.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.9.3->together==1.3.14) (1.4.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\user\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.9.3->together==1.3.14) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.9.3->together==1.3.14) (1.11.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\user\\anaconda3\\lib\\site-packages (from click<9.0.0,>=8.1.7->together==1.3.14) (0.4.6)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from pydantic<3.0.0,>=2.6.3->together==1.3.14) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in c:\\users\\user\\anaconda3\\lib\\site-packages (from pydantic<3.0.0,>=2.6.3->together==1.3.14) (2.27.2)\n",
      "Requirement already satisfied: typing-extensions>=4.12.2 in c:\\users\\user\\anaconda3\\lib\\site-packages (from pydantic<3.0.0,>=2.6.3->together==1.3.14) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\user\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.31.0->together==1.3.14) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\user\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.31.0->together==1.3.14) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.31.0->together==1.3.14) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\user\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.31.0->together==1.3.14) (2024.12.14)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from rich<14.0.0,>=13.8.1->together==1.3.14) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from rich<14.0.0,>=13.8.1->together==1.3.14) (2.15.1)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from typer<0.16,>=0.9->together==1.3.14) (1.5.4)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from markdown-it-py>=2.2.0->rich<14.0.0,>=13.8.1->together==1.3.14) (0.1.0)\n",
      "Downloading together-1.3.14-py3-none-any.whl (73 kB)\n",
      "Downloading eval_type_backport-0.2.2-py3-none-any.whl (5.8 kB)\n",
      "Downloading rich-13.9.4-py3-none-any.whl (242 kB)\n",
      "Installing collected packages: eval-type-backport, rich, together\n",
      "  Attempting uninstall: rich\n",
      "    Found existing installation: rich 13.7.1\n",
      "    Uninstalling rich-13.7.1:\n",
      "      Successfully uninstalled rich-13.7.1\n",
      "Successfully installed eval-type-backport-0.2.2 rich-13.9.4 together-1.3.14\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install together==1.3.14 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ac79fd23-4070-4bf0-8563-3c614702b6fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting python-dotenv==1.0.1\n",
      "  Downloading python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\n",
      "Downloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
      "Installing collected packages: python-dotenv\n",
      "  Attempting uninstall: python-dotenv\n",
      "    Found existing installation: python-dotenv 0.21.0\n",
      "    Uninstalling python-dotenv-0.21.0:\n",
      "      Successfully uninstalled python-dotenv-0.21.0\n",
      "Successfully installed python-dotenv-1.0.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install python-dotenv==1.0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7ec6e5a-d388-4288-944c-d2e0dd054e60",
   "metadata": {},
   "source": [
    "### Importing Required Libraries\n",
    "\n",
    "import all required libraries:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "15b3f988-d3ee-4ffd-8f2d-6c86f27b0763",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from together import Together"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b65920f-d2ff-4cd3-a61f-4eac2d224257",
   "metadata": {},
   "source": [
    "## Background\n",
    "\n",
    "### 1. What is DeepSeek?\n",
    "\n",
    "DeepSeek is a type of **Large Language Model (LLM)**‚Äîan artificial intelligence system designed to understand and generate human-like text. Developed by a Chinese company, DeepSeek uses advanced techniques to perform tasks such as answering questions, writing stories, solving math problems, and even helping with coding.\n",
    "\n",
    "- **Large Language Model (LLM):** A computer program trained on huge amounts of text data to learn how language works.\n",
    "- **Open Source:** DeepSeek is released with open-source code, which means anyone can view, use, and modify it.\n",
    "\n",
    "### 2. What Makes DeepSeek Special Compared to Other LLMs?\n",
    "\n",
    "DeepSeek has a few unique features that set it apart from many other language models:\n",
    "\n",
    "- **Cost-Efficient Training:**  \n",
    "  It is built using fewer computing resources compared to many other prominent models. This means it can be developed and maintained at a much lower cost.\n",
    "\n",
    "- **Advanced Reasoning Capabilities:**  \n",
    "  DeepSeek is designed to handle complex tasks (like math and coding) by sometimes showing its \"chain of thought\"‚Äîa step-by-step process that explains how it reaches an answer. Although you might not always want to see this internal reasoning, it can be useful for debugging and learning.\n",
    "\n",
    "- **Open-Source Nature:**  \n",
    "  Unlike some proprietary models, DeepSeek is open source. This transparency allows researchers and developers to experiment with the model and build customized applications without having to pay for expensive licenses.\n",
    "\n",
    "- **Efficient Use of Resources:**  \n",
    "  DeepSeek employs a technique called **Mixture-of-Experts (MoE)**. This means that, even though the model has billions of parameters (the parts of the model that learn from data), only a small portion is activated for any given task‚Äîsaving both time and computational power.\n",
    "\n",
    "### 3. How Does DeepSeek Work?\n",
    "\n",
    "DeepSeek works in a similar way to other modern language models but includes some special techniques:\n",
    "\n",
    "- **Transformer Architecture:**  \n",
    "  Like many LLMs, DeepSeek is built on the transformer architecture. This design allows the model to process input text in layers and understand the relationships between words.\n",
    "\n",
    "- **Training on Massive Data:**  \n",
    "  The model is trained on a vast amount of text data (often billions or even trillions of words). By doing so, it learns patterns in language‚Äîsuch as grammar, facts, and common reasoning steps.\n",
    "\n",
    "- **Mixture-of-Experts (MoE):**  \n",
    "  Instead of using all of its billions of parameters at once, DeepSeek uses a method called MoE to activate only the parts of the model needed for the current task. This makes it faster and cheaper to run.\n",
    "\n",
    "- **Chain-of-Thought Reasoning:**  \n",
    "  For complex tasks, DeepSeek can generate a visible ‚Äúchain-of-thought‚Äù where it outlines its reasoning process. This is similar to thinking out loud step-by-step, which can help in understanding how it arrives at an answer.\n",
    "\n",
    "- **Learning Through Reinforcement:**  \n",
    "  DeepSeek improves its responses by using reinforcement learning‚Äîa process where the model is rewarded for giving correct or helpful answers, encouraging it to learn from mistakes.\n",
    "\n",
    "Together, these components allow DeepSeek to be both powerful and efficient. Its design helps make advanced AI more accessible to developers and researchers around the world.\n",
    "\n",
    "Read more about on DeepSeek on this [link](https://www.ibm.com/think/news/deepseek-r1-ai?utm_source=skills_network&utm_content=in_lab_content_link&utm_id=Lab-deepsake_r1-v1_1738787911).\n",
    "\n",
    "You can read the research paper on [ArXiv](https://arxiv.org/pdf/2501.12948).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a34c5d08-9f10-475c-bf8d-516a5a6fcb73",
   "metadata": {},
   "source": [
    "<img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/AC65JS7pFWRGvLx11fYUOg/img.png\" width=\"50%\" alt=\"Restart kernel\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57210eef-0918-40bc-90e2-30fdac809621",
   "metadata": {},
   "source": [
    "## Getting Started with the Lab  \n",
    "\n",
    "Before you begin this lab, please follow these steps to obtain your API key for DeepSeek R1:  \n",
    "\n",
    "1. **Visit Together.ai:**  \n",
    "   - Go to the [together.ai DeepSeek R1 page](https://www.together.ai/models/deepseek-r1).  \n",
    "   - Click on **\"Try our DeepSeek R1 API\"**.  \n",
    "\n",
    "2. **Register for an Account:**  \n",
    "   - You will be prompted to register on the website.  \n",
    "   - Once you log in, you will land on the **\"Playground\"** section.  \n",
    "   - Click on the **\"Dashboard\"** tab.  \n",
    "   - Scroll down, and you will find your **hidden API key**, copy it.  \n",
    "   \n",
    "3. **Receive Free Credit:**  \n",
    "   - Together.ai provides you with a free **$1 credit** that you can use for making API calls to DeepSeek R1.  \n",
    "   - Be mindful when using your credits, as any charges incurred after your free credit is used will be your responsibility.  \n",
    "   - Always check your available credits before making API calls to avoid unexpected charges.\n",
    "\n",
    "4. **Configure Your Lab Environment:**  \n",
    "   - Open the `file.env` file located in the file library on the left (near the Skills Network logo). If it's not there after downloading it (after running the below code), please hit `refresh` icon just above the file name (and not the whole page).  \n",
    "   - Paste your API key into the file, e.g.:  \n",
    "     ```\n",
    "     TOGETHER_API_KEY=<YOUR_API_KEY_HERE>\n",
    "     ```\n",
    "   - Save the file.  \n",
    "\n",
    "5. **Load the API Key in Your Lab Code:**  \n",
    "   - In your lab scripts, the API key will be loaded using `load_dotenv` and `os.environ.get(\"TOGETHER_API_KEY\")`.  \n",
    "   - This will allow your code to access the DeepSeek R1 API.  \n",
    "\n",
    "> Be mindful when using your credits, as any charges incurred after your free credit is used will be your responsibility. Always check your available credits before making API calls to avoid unexpected charges. \n",
    "\n",
    "Follow these steps to set up your environment, and you'll be ready to start exploring DeepSeek R1 in no time in this lab!  \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1f529500-f528-467c-9831-adcae9486fcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Download successful!\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "url = \"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/1Vt7q-XX3Frj5AtEpTsK2g/file.env\"\n",
    "response = requests.get(url)\n",
    "\n",
    "if response.status_code == 200:\n",
    "    with open(\"file.env\", \"wb\") as file:\n",
    "        file.write(response.content)\n",
    "    print(\"Download successful!\")\n",
    "else:\n",
    "    print(f\"Failed to download file. Status code: {response.status_code}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "534012a6-bd70-4e09-ba8d-6cd52c928b3e",
   "metadata": {},
   "source": [
    "First, let's download the file.env file,this gives you all the stuff you need to run DeepSeek. You can either run the following command (which downloads the file for convenience) or create a new file.env file manually:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e096a4f-7c86-4ee6-bed3-182f448746ac",
   "metadata": {},
   "source": [
    "After that, we initialize a `Together` client. This client acts like a friendly messenger between our code and the DeepSeek R1 API. It takes our requests (like asking a question or asking for a code snippet), sends them to DeepSeek, and then collects and returns the responses for us. Using this client means we don't have to write all the low-level code to handle network connections and HTTP requests ourselves‚Äîit takes care of those details, allowing us to focus on solving our problem.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2140f625-c585-451a-881b-41f41d769c66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables from file.env\n",
    "load_dotenv('file.env')\n",
    "\n",
    "# Retrieve the API key from the environment (optional if the library auto-detects)\n",
    "api_key = os.environ.get(\"TOGETHER_API_KEY\")\n",
    "if api_key is None:\n",
    "    raise ValueError(\"API key not found. Make sure TOGETHER_API_KEY is set in file.env\")\n",
    "\n",
    "# Initialize the client without passing the API key directly\n",
    "client = Together()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de6f074d-b6aa-47b9-b205-f778576ab6c6",
   "metadata": {},
   "source": [
    "Let's check our event variable    ```TOGETHER_API_KEY```  to make sure it its set correctly \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e9571f58-e3ab-4551-b158-fdb18e219adc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "$TOGETHER_API_KEY\n"
     ]
    }
   ],
   "source": [
    "!echo $TOGETHER_API_KEY"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3180a750-666d-48cb-8a4d-0123954d0ca7",
   "metadata": {},
   "source": [
    "## Using DeepSeek for Everyday Tasks\n",
    "\n",
    "First, let's see how we can use DeepSeek to solve a normal task‚Äîlike finding fun things to do in New York.\n",
    "\n",
    "In this example, we create a response message using `client.chat.completions.create`:\n",
    "\n",
    "- **model:**  \n",
    "  Specifies which AI model to use (here, \"deepseek-ai/DeepSeek-R1\").\n",
    "\n",
    "- **messages:**  \n",
    "  A list of messages that form the conversation. We provide a single message from the user asking for fun activities in New York.\n",
    "\n",
    "- **temperature:**  \n",
    "  Controls the creativity of the response. If the temperature is 0 or close to 0, model provides focused and consistent answers. If it is 1 or near 1, model provides creative and diverse responses. A value of 0.7 means the answer will be reasonably creative yet focused. \n",
    "\n",
    "- **max_tokens:**  \n",
    "  Sets the maximum length of the generated response (in this case, up to 1000 tokens).\n",
    "\n",
    "- **top_p:**  \n",
    "   This parameter, also known as nucleus sampling, restricts the model to only consider a subset of tokens for each prediction. A top_p value of 0.9 means that at each step, the model will only choose from the smallest group of tokens whose combined probability is at least 90%. This helps keep the output both varied and coherent by filtering out very unlikely words.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "798d1ab7-e686-4983-8393-b866d7ad4329",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think>\n",
      "Okay, the user asked for a joke. Let me think of something light-hearted and not offensive. Maybe a classic setup with a pun or a twist. Animals are usually safe. How about something with a dog? Or maybe a cat? Wait, I remember a joke about a dog and a phone. Let me check if that's appropriate. The punchline is \"He‚Äôs a golden retriever!\" because retrievers fetch things. Yeah, that's a good pun. No sensitive topics, no offensive content. Should work. Alright, let me phrase it clearly.\n",
      "</think>\n",
      "\n",
      "Sure! Here's a lighthearted one:  \n",
      "\n",
      "Why did the dog sit in the shade during the phone call?  \n",
      "‚Ä¶Because he didn‚Äôt want to be a *hot dog*!  \n",
      "\n",
      "*(Bonus groan: Also, he heard it‚Äôs rude to bark in the sun.)* üòÑ"
     ]
    }
   ],
   "source": [
    "from together import Together\n",
    "\n",
    "client = Together(api_key=\"ace9de1bf2f417e8a9a512e1b6b22827adb31a2735179b34cd4e9803c08eb591\")  # Replace with your API key\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"deepseek-ai/DeepSeek-R1\",\n",
    "    messages=[{\"role\": \"user\", \"content\": \"Tell me a joke\"}],  # Ensure messages are not empty\n",
    "    max_tokens=None,\n",
    "    temperature=0.6,\n",
    "    top_p=0.95,\n",
    "    top_k=50,\n",
    "    repetition_penalty=1,\n",
    "    stop=[\"<ÔΩúend‚ñÅof‚ñÅsentenceÔΩú>\"],\n",
    "    stream=True\n",
    ")\n",
    "\n",
    "for token in response:\n",
    "    if hasattr(token, 'choices'):\n",
    "        print(token.choices[0].delta.content, end='', flush=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1915659-cf4c-45ee-a6d5-19d2fa85b0a5",
   "metadata": {},
   "source": [
    "### Understanding the Output Structure\n",
    "\n",
    "When you run the code, you might notice that the output starts with a section wrapped in `<think> ... </think>`. This part is known as the **chain-of-thought**. Here‚Äôs what that means:\n",
    "\n",
    "- **Chain-of-Thought:**  \n",
    "  This is the internal reasoning process of the LLM. It shows the step-by-step internal thought process the model goes through to arrive at the final answer. Although it‚Äôs very detailed (and can use up many of the allowed output tokens), it is generally not intended for the end user.\n",
    "\n",
    "- **Final Answer:**  \n",
    "  After the chain-of-thought section (which ends with `</think>`), you see the concise final answer. This final part is the result that the model presents to you as the solution.\n",
    "\n",
    "**Note:**  \n",
    "- The chain-of-thought is an integral part of how the model works, and the 1000 output tokens include both the reasoning and the final answer.\n",
    "- Every time you run the code, the exact content of the chain-of-thought may vary because the model generates a new internal reasoning process for each query. However, the structure remains the same: a `<think> ... </think>` block followed by the final response.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80bd33d8-89d8-4547-8e25-a008747cadc3",
   "metadata": {},
   "source": [
    "## Generating Code Completion with Streaming\n",
    "\n",
    "Next, let's look at how we generate code‚Äîfor example, completing a Fibonacci function‚Äîusing DeepSeek. In this case, we include an extra parameter called `stream` in our API call:\n",
    "\n",
    "- **stream:**  \n",
    "  Setting `stream=True` tells the API to send the output incrementally in small chunks rather than waiting until the full response is generated. This is especially useful when you want to display results in real time, such as when debugging or observing how the response builds up.\n",
    "\n",
    "Because the output is received in pieces, we change the way we print the response. Instead of printing a single block of text, we loop through each chunk and print its content as it arrives. This loop also includes error handling (using a try/except block) to skip any chunks that might not contain the expected data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2a4bb99b-694e-4d29-8593-f93926e3be5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Code Completion:\n",
      "\\n    if n <= 0:\\n        return 0\\n    elif n == 1:\\n        return 1\\n    else:\\n        return fibonacci(n-1) + fibonacci(n-2)\\n\\nprint(fibonacci(10))\", \"answer\": 55, \"thought\": \"The code defines a recursive Fibonacci function. For n=10, it calculates the 10th Fibonacci number. The sequence starts with 0 and 1, so the "
     ]
    }
   ],
   "source": [
    "# Generate code completion for a Fibonacci function snippet.\n",
    "code_response = client.completions.create(\n",
    "    model=\"deepseek-ai/DeepSeek-R1\",\n",
    "    prompt=\"def fibonacci(n):\",\n",
    "    temperature=0,  # Lower temperature for a more predictable completion.\n",
    "    max_tokens=100,   # Limit the completion length.\n",
    "    top_p=1.0,        # Full distribution sampling.\n",
    "    stream=True,      # Enable streaming to see incremental output.\n",
    ")\n",
    "\n",
    "print(\"\\nCode Completion:\")\n",
    "for chunk in code_response:\n",
    "    try:\n",
    "        # Attempt to print the text content\n",
    "        print(chunk.choices[0].text or \"\", end=\"\", flush=True)\n",
    "    except (IndexError, AttributeError):\n",
    "        # If the chunk doesn't contain the expected data, skip it.\n",
    "        continue"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42b05b20-8ee2-4312-baa0-83db5daa2735",
   "metadata": {},
   "source": [
    "### Understanding the Code Completion Output\n",
    "\n",
    "In this example, we generate a code completion for a Fibonacci function using the DeepSeek R1 API with streaming enabled. Notice that when streaming is turned on, we only see the final code output without any internal chain-of-thought (the reasoning process).\n",
    "\n",
    "- **Streaming Enabled:**  \n",
    "  By setting `stream=True`, the API sends the response incrementally in small chunks. For code completions, the model is optimized to provide the final, ready-to-use code directly, without including the chain-of-thought that you might see in conversational responses.\n",
    "\n",
    "- **Why No Chain-of-Thought for Code?**  \n",
    "  In code completions, the focus is on delivering a precise, functional snippet. The chain-of-thought‚Äîdetailing all the internal reasoning steps‚Äîis generally not needed and is omitted to keep the output concise and relevant. This is why you only see the completed code without the extra internal reasoning.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43dc844c-2842-48d1-9a05-58e6c8fccb48",
   "metadata": {},
   "source": [
    "## Generating a Custom Chat Response\n",
    "\n",
    "In this example, we use the `client.chat.completions.create` method to ask DeepSeek to explain recursion in Python in a single paragraph. We adjust several parameters to fine-tune the response:\n",
    "\n",
    "- **temperature:**  \n",
    "  Set to `1.0` to encourage a more creative and less deterministic response.\n",
    "\n",
    "- **max_tokens:**  \n",
    "  Set to `500` to allow for a longer and more detailed explanation, while still keeping it to a single paragraph.\n",
    "\n",
    "- **top_p:**  \n",
    "  Set to `1.0` so that the model considers the full range of token probabilities for maximum diversity in its output.\n",
    "\n",
    "The code snippet below demonstrates how to generate and print this custom chat response:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e5f52bf7-a791-46c0-a2a5-c10c4ca71b77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Custom Chat Response (High Temperature):\n",
      " <think>\n",
      "Okay, the user wants me to explain recursion in Python in a single paragraph. Let's start by recalling what recursion is. It's a function calling itself until it meets a base condition. I need to make sure to mention the base case and the recursive step.\n",
      "\n",
      "Wait, right, without a base case, the recursion would go on infinitely, causing a stack overflow. So the example should probably include a simple case to show both components. Maybe factorial or Fibonacci? Factorial is straightforward.\n",
      "\n",
      "Should I use an example in the paragraph or just describe it? The user wants a concise explanation, so maybe a brief code snippet would help. Like def factorial(n): if n == 0: return 1 else: return n * factorial(n-1). But including code in a paragraph might be messy. Instead, mention the components without writing the whole code.\n",
      "\n",
      "Also, need to highlight that each recursive call reduces the problem size, approaching the base case. Mention potential issues like stack overflow for deep recursion, but keep it brief. Maybe note that recursion is elegant but not always efficient compared to iteration.\n",
      "\n",
      "So structure: definition, base case, recursive step, example hint, advantage and caution. Keep it one paragraph, concise but covers all key points. Check for clarity. Avoid jargon where possible.\n",
      "</think>\n",
      "\n",
      "Recursion in Python occurs when a function calls itself directly or indirectly to solve a problem, repeatedly breaking it into smaller subproblems until a **base case** (a terminating condition) is reached, which stops further recursive calls. Each recursive call works on a reduced version of the original problem (e.g., decrementing a counter or slicing a list), gradually approaching the base case, and returns results back up the call stack to compute the final output. While recursion simplifies code for problems like tree traversals or factorials, improper handling of the base case or excessive depth can lead to stack overflow errors, making iterative approaches preferable for performance-critical tasks.\n"
     ]
    }
   ],
   "source": [
    "# Example: Ask the model to explain recursion in Python, but change parameters to see different styles.\n",
    "custom_response = client.chat.completions.create(\n",
    "    model=\"deepseek-ai/DeepSeek-R1\",\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": \"Explain recursion in Python in single paragraph.\"}\n",
    "    ],\n",
    "    temperature=1.0,    # Increase creativity and randomness in the response.\n",
    "    max_tokens=500,     # Allow for a longer, more detailed explanation.\n",
    "    top_p=1.0,\n",
    ")\n",
    "print(\"\\nCustom Chat Response (High Temperature):\\n\", custom_response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec5409b3-ddfb-4e96-b8dd-aaa19e60ec2b",
   "metadata": {},
   "source": [
    "## Generating a Custom Chat Response with Parameter Tuning\n",
    "\n",
    "In this example, we define a helper function called `get_response()` that sends a prompt to the DeepSeek R1 API and returns the final answer. This function lets us easily experiment with different parameter settings to see how they affect the output. Here's a quick breakdown of the key parameters:\n",
    "\n",
    "- **temperature:**  \n",
    "  Controls the creativity of the response. A higher temperature (like 1.0) makes the output more varied and creative, while a lower temperature makes it more predictable.\n",
    "\n",
    "- **max_tokens:**  \n",
    "  Sets the maximum number of tokens (words and punctuation) the model can generate. A higher value (e.g., 500) allows for a longer, more detailed answer, while a lower value (e.g., 50) limits the response to a shorter format.\n",
    "\n",
    "- **top_p:**  \n",
    "  This parameter, also known as nucleus sampling, determines the diversity of the output. When set to 1.0, the model considers the full range of token probabilities for maximum diversity. By adjusting this value, you can control how focused or varied the output is.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "115362c2-bb66-40e2-96cc-444782be0b41",
   "metadata": {},
   "source": [
    "> One of the most exciting parts of working with any LLM is how you can tweak different parameters to see how the output changes and Deepseek is no different. For instance, try changing the `top_p` value to adjust the diversity of the generated text, or add the `stream` parameter to see your results in real time. Each change can make the model behave in unique and interesting ways!\n",
    "\n",
    "> Below is an example of a helper function that allows you to easily test different settings:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4383cae4-76b7-4664-83e6-20c6166a4536",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can create a function to easily experiment with various parameter settings.\n",
    "def get_response(prompt, temperature=0.7, max_tokens=500, top_p=0.9):\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"deepseek-ai/DeepSeek-R1\",\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        temperature=temperature,\n",
    "        max_tokens=max_tokens,\n",
    "        top_p=top_p,\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "# Try different parameter values and compare the outputs.\n",
    "prompt_text = \"What are loops in Python?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6b75c618-c990-4f9b-8e8a-a5f3cf47eeda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Default Params:\n",
      " <think>\n",
      "Okay, so I need to explain what loops are in Python. Let me start by recalling what I know. Loops are a fundamental concept in programming that allow you to repeat a block of code multiple times. In Python, there are two main types of loops: for loops and while loops. \n",
      "\n",
      "First, for loops. I think they're used when you know how many times you want to loop, like iterating over a list or a range. For example, if you have a list of numbers and you want to print each one, a for loop would be perfect. The syntax starts with \"for\" followed by a variable name, then \"in\", and then the iterable object. The code block under the for statement is indented, right? So like:\n",
      "\n",
      "for number in [1, 2, 3]:\n",
      "    print(number)\n",
      "\n",
      "That would print each number on a new line. Also, using range() with for loops is common. Range can generate a sequence of numbers, which is useful for looping a specific number of times. Like range(5) gives 0 to 4, so looping 5 times.\n",
      "\n",
      "Then there's the while loop. While loops are used when you want to repeat a block of code as long as a condition is true. For instance, if you want to keep asking for user input until they enter a valid response, a while loop would work. The syntax is \"while\" followed by a condition, then a colon. The indented code beneath runs as long as the condition is true. You have to be careful with while loops to avoid infinite loops. For example:\n",
      "\n",
      "count = 0\n",
      "while count < 5:\n",
      "    print(count)\n",
      "    count += 1\n",
      "\n",
      "This would print numbers 0 through 4. If you forget to increment count, the loop would run forever.\n",
      "\n",
      "Control statements like break and continue are important too. Break exits the loop immediately, while continue skips the rest of the current iteration and moves to the next one. For example, in a for loop, if you have a condition that when met, you break out of the loop, that would stop the looping entirely. Continue would skip any code below it for the current iteration and go to the next item.\n",
      "\n",
      "Also, else clauses can be used with loops. I remember that in Python, you can have an else block after a loop, which runs when the loop completes normally (not stopped by a break). For example:\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nDefault Params:\\n\", get_response(prompt_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d455076f-e1aa-4767-bb19-c7b149fa362d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "High Temperature (more creative):\n",
      " <think>\n",
      "Okay, let's see. The user is asking about loops in Python. Hmm, where to start? I remember that loops are used to repeat code, right? But wait, there are different types of loops in Python. Like, for loops and while loops. Yeah, those are the two main ones.\n",
      "\n",
      "First, for loops. They iterate over a sequence, right? Like a list, tuple, dictionary, set, or string. So for each item in the sequence, the loop runs the block of code. Oh, right, the syntax is for variable in sequence: then indented code block. Like for example, looping through a list of numbers and printing each one.\n",
      "\n",
      "Then there's the range() function. Oh yeah, that's commonly used with for loops. For example, range(5) gives numbers from 0 to 4. So you can loop a specific number of times. Maybe the user might want an example of that too.\n",
      "\n",
      "While loops, on the other hand, run as long as a condition is true. So you set a condition, and the loop keeps executing until that condition becomes false. But you have to be careful with while loops because they can cause infinite loops if the condition never becomes false. Like, if you forget to increment the variable in the loop.\n",
      "\n",
      "Let me think of examples. For a for loop, maybe something simple like printing each fruit in a list. For a while loop, maybe counting down from 5 to 1. That would show the condition changing each iteration.\n",
      "\n",
      "Oh, and there's also control statements like break and continue. Break exits the loop immediately, continue skips the rest of the current iteration and moves to the next one. Also, the else clause in loops, which I think runs when the loop finishes normally, not if it's exited by a break. That's a bit less common but good to mention.\n",
      "\n",
      "Nested loops could be another point. Like having a loop inside another loop. For example, iterating over a grid's rows and columns.\n",
      "\n",
      "Wait, maybe also mention the pass statement. Sometimes you need a placeholder in a loop, and pass does nothing. Useful for avoiding syntax errors when you haven't implemented the loop body yet.\n",
      "\n",
      "I should structure the answer starting with the definition of loops, then for and while loops with syntax and examples. Then mention control statements and maybe nested loops. Oh, and efficiency considerations? Like, using loops appropriately without unnecessary complexity.\n",
      "\n",
      "Let me\n"
     ]
    }
   ],
   "source": [
    "# Keeping temperature to 1\n",
    "print(\"\\nHigh Temperature (more creative):\\n\", get_response(prompt_text, temperature=1.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d67efce6-706b-4e38-a71b-94a94b9f6896",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Low Max Tokens (shorter response):\n",
      " <think>\n",
      "Okay, so I need to understand what loops are in Python. Let me start by recalling what I know about programming in general. Loops are structures that let you repeat a block of code multiple times. That makes sense because sometimes you need\n"
     ]
    }
   ],
   "source": [
    "# Setting max tokens to 50\n",
    "print(\"\\nLow Max Tokens (shorter response):\\n\", get_response(prompt_text, max_tokens=50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a9cb848-0500-4f23-8bd0-b39b74517c69",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
